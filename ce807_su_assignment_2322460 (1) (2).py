# -*- coding: utf-8 -*-
"""CE807-SU-Assignment_2322460.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iGT3cYfDQCqVwME75e89ibgD6keeKeFQ
"""

import pandas as pd
import numpy as np
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import joblib
import os
from google.colab import drive

# Mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

# Add your code to initialize GDrive and data and models paths

GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = './CE807-24-SU/Assignment/'
GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))

# Student ID and dataset number
student_id = '2322460'
dataset_number = '41'

# Set seed for reproducibility
np.random.seed(int(student_id))

# Define file paths
data_path = os.path.join(GOOGLE_DRIVE_PATH, 'data', '0') #  last digit of  Regitration number
model_unsup_path = f'./model/{student_id}/Model_unsup/'
model_dis_path = f'./model/{student_id}/Model_dis/'

# Create directories if they don't exist
os.makedirs(model_unsup_path, exist_ok=True)
os.makedirs(model_dis_path, exist_ok=True)

# Preprocessing Function
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize stop words and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    filtered_tokens = [lemmatizer.lemmatize(w) for w in tokens if w.isalnum() and w not in stop_words]
    return ' '.join(filtered_tokens)

#  counting the total number of rows in the train.csv and number of positive and negative

import pandas as pd
# Load the training data
train_df = pd.read_csv(os.path.join(GOOGLE_DRIVE_PATH, 'data', '0', 'train.csv'))

# Count the total number of rows
total_rows = len(train_df)

# Count the number of positive and negative instances
positive_count = train_df['sentiment'].value_counts()['positive']
negative_count = train_df['sentiment'].value_counts()['negative']

print("Total number of rows:", total_rows)
print("Number of positive instances:", positive_count)
print("Number of negative instances:", negative_count)

# LDA Model (Unsupervised)
# Load and preprocess data
train_data = pd.read_csv(os.path.join(data_path, 'train.csv'))
print(train_data.head())
print(train_data.count())

train_data['processed_text'] = train_data['text'].apply(preprocess_text)

# Convert text to bag-of-words representation
vectorizer = CountVectorizer(max_features=1000)
train_bow = vectorizer.fit_transform(train_data['processed_text'])

# Train LDA model
lda = LatentDirichletAllocation(n_components=10, random_state=int(student_id))
lda.fit(train_bow)

# Save the model and vectorizer
joblib.dump(lda, os.path.join(model_unsup_path, 'lda_model.pkl'))
joblib.dump(vectorizer, os.path.join(model_unsup_path, 'vectorizer.pkl'))

# Load the training data
val_df = pd.read_csv(os.path.join(GOOGLE_DRIVE_PATH, 'data', '0', 'valid.csv'))

# Count the total number of rows
total_rows = len(val_df)

# Count the number of positive and negative instances
positive_count = val_df['sentiment'].value_counts()['positive']
negative_count = val_df['sentiment'].value_counts()['negative']

print("Total number of rows:", total_rows)
print("Number of positive instances:", positive_count)
print("Number of negative instances:", negative_count)

# SVM Model (Discriminative)
# Load and preprocess data
train_data['processed_text'] = train_data['text'].apply(preprocess_text)

# Train-test split
X_train, X_valid, y_train, y_valid = train_test_split(train_data['processed_text'], train_data['sentiment'], test_size=0.2, random_state=int(student_id))

# Create a pipeline with TF-IDF and SVM
pipeline = make_pipeline(TfidfVectorizer(max_features=1000), SVC(kernel='linear', random_state=int(student_id)))
pipeline.fit(X_train, y_train)

# Save the model
joblib.dump(pipeline, os.path.join(model_dis_path, 'svm_model.pkl'))

# Evaluate the model
y_pred = pipeline.predict(X_valid)
print(classification_report(y_valid, y_pred))

import pandas as pd
# Load the training data
test_df = pd.read_csv(os.path.join(data_path, 'test.csv'))

# Count the total number of rows
total_rows = len(test_df)

print(total_rows)

# Combining LDA and SVM for Extra Credit
test_data = pd.read_csv(os.path.join(data_path, 'test.csv'))
test_data['processed_text'] = test_data['text'].apply(preprocess_text)
# Extract TF-IDF features for the training data
tfidf_features_train = pipeline.named_steps['tfidfvectorizer'].transform(X_train)

# Get LDA topics as features for the training data
train_bow = vectorizer.transform(X_train)
lda_topics_train = lda.transform(train_bow)

# Combine TF-IDF features and LDA topics for training
combined_features_train = np.hstack([tfidf_features_train.toarray(), lda_topics_train])
# Retrain the SVM model on the combined features
combined_svm = SVC(kernel='linear', random_state=int(student_id))
combined_svm.fit(combined_features_train, y_train)

# Save the retrained combined model
joblib.dump(combined_svm, os.path.join(model_dis_path, 'combined_svm_model.pkl'))
# Extract TF-IDF features for the validation data
tfidf_features_valid = pipeline.named_steps['tfidfvectorizer'].transform(X_valid)

# Get LDA topics as features for the validation data
valid_bow = vectorizer.transform(X_valid)
lda_topics_valid = lda.transform(valid_bow)

# Combine TF-IDF features and LDA topics for validation
combined_features_valid = np.hstack([tfidf_features_valid.toarray(), lda_topics_valid])

# Predict using the combined SVM model
y_valid_pred = combined_svm.predict(combined_features_valid)
print(classification_report(y_valid, y_valid_pred))
# Extract TF-IDF features for the test data
tfidf_features_test = pipeline.named_steps['tfidfvectorizer'].transform(test_data['processed_text'])

# Get LDA topics as features for the test data
test_bow = vectorizer.transform(test_data['processed_text'])
lda_topics_test = lda.transform(test_bow)

# Combine TF-IDF features and LDA topics for test data
combined_features_test = np.hstack([tfidf_features_test.toarray(), lda_topics_test])

# Predict using the combined SVM model
svm_predictions = combined_svm.predict(combined_features_test)

# Save the output
test_data['out_label_model_unsup'] = np.argmax(lda_topics_test, axis=1)
test_data['out_label_model_dis'] = svm_predictions

test_data.drop(columns=['processed_text'], inplace=True)

test_data.to_csv(os.path.join(data_path, 'test.csv'), index=False)